{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,Dropout,Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.optimizers import SGD,Adam,RMSprop\n",
    "import pandas as pd\n",
    "import time\n",
    "from tensorflow.keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hessianfree as hf\n",
    "from hessianfree.loss_funcs import LossFunction\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO TURN GPU for Keras, set devic = cuda or gpu or gpu0 like this\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=cuda,openmp=1,floatX=float32\" \n",
    "# TO TURN ON OPENMP\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=cpu,openmp=1,floatX=float32\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_NN(n_nodes,optimizer):\n",
    "    '''This function initializes and return a new neural network with regularization techniques\n",
    "       \n",
    "       input: \n",
    "       n_nodes: a list of units per layer like [42,24,12,1] \n",
    "       optimizer: one of the following:\n",
    "        sgd = SGD\n",
    "        rmsprop = RMSprop\n",
    "        adagrad = Adagrad\n",
    "        adadelta = Adadelta\n",
    "        adam = Adam\n",
    "        adamax = Adamax\n",
    "        nadam = Nadam\n",
    "       \n",
    "\n",
    "       output: an object that contains these methods:\n",
    "       \n",
    "       model.predict(X): return predictions corresponding to X\n",
    "       \n",
    "       model.get_weights(): return a list of current model weights, in the order of w0,b1,w1,b1,....w4,b4\n",
    "       \n",
    "       model.set_weights(): takes in a list of weights in the same format as what model.get_weights() returns\n",
    "       \n",
    "       model.fit(X_tr,Y_tr,verbose=0,epochs=50,batch_size=1024,validation_split=0.2, callbacks=[early_stopping]): \n",
    "       \n",
    "       train a model with the inputs and the specification, you can train 1 epoch;  \n",
    "       and return history of loss during training (using hist.history['loss']) and validation loss if callbacks =\n",
    "       [EarlyStopping(patience=5)] (using hist.history['val_loss']) \n",
    "       \n",
    "    '''\n",
    "    # Clear the model\n",
    "    model = None\n",
    "    # BUILD INPUT LAYER\n",
    "    inputs = Input(shape=(n_nodes[0],))\n",
    "\n",
    "    # CONNECT TO THE FIRST HIDDEN LAYER\n",
    "    x = Dense(n_nodes[1], kernel_initializer='he_normal', \n",
    "                    kernel_regularizer=l2(0.0001),kernel_constraint = max_norm(5), activation='relu')(inputs)\n",
    "    x = Dropout(0.2)(x) # add dropout \n",
    "\n",
    "    # ADD SOME MORE HIDDEN LAYERS\n",
    "    for i in range(2,len(n_nodes)-1):\n",
    "        x = Dense(n_nodes[i],  kernel_initializer='he_normal', activation='relu',bias_initializer='he_normal',\n",
    "            kernel_regularizer=l2(0.0001),kernel_constraint = max_norm(3))(x)\n",
    "        x = Dropout(0.2)(x) # add dropout \n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    predictions = Dense(1, kernel_initializer='he_normal', activation='linear')(x)\n",
    "\n",
    "    # INITIALIZE MODEL (now you can call model.get_weights() )\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # Compile model with LOSS FUNCTION and ADAM OPTIMIZER\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of total X and ret: (19669, 42) (19669, 1)\n"
     ]
    }
   ],
   "source": [
    "# Example OF comparing keras and Hessian Free: \n",
    "\n",
    "# read data and define training, validation and test set\n",
    "data = np.genfromtxt('price_inputs_GS2016.csv',delimiter=',',skip_header=1)\n",
    "X,ret = data[:,2:],data[:,1:2] # X means features, ret means target \n",
    "print('shape of total X and ret:',X.shape,ret.shape)\n",
    "\n",
    "n_test = int(X.shape[0]*0.25)\n",
    "N = X.shape[0] - n_test\n",
    "n_val = int(N*0.2)\n",
    "X_tr_temp, X_test, ret_tr_temp,ret_test = X[:-n_test],X[-n_test:],ret[:-n_test],ret[-n_test:]\n",
    "X_tr,X_val,ret_tr,ret_val = X_tr_temp[:-n_val], X_tr_temp[-n_val:],ret_tr_temp[:-n_val],ret_tr_temp[-n_val:]\n",
    "\n",
    "\n",
    "# define evaluation metrics\n",
    "accuracy = lambda pred,truth: np.mean((pred>0)==(truth>0))\n",
    "hit_ratio = lambda x,y: np.mean( ((x[1:] - x[:-1]) * (y[1:]-y[:-1]))>0 )\n",
    "eval_f = [accuracy,hit_ratio,mean_squared_error,mean_absolute_error]\n",
    "labels = 'accuracy,hit_ratio,mean_squared_error,mean_absolute_error'.split(',')\n",
    "\n",
    "n_trials = 1 # run some number of trials for each model for confidence interval \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fitting on the training set for 100 epochs, keras return this weight parameter\n",
      "[array([[-0.1533167 ,  0.21487777,  0.18682274, ...,  0.1323001 ,\n",
      "         0.00276681,  0.17073455],\n",
      "       [-0.0922346 ,  0.1681947 , -0.10161486, ..., -0.07924437,\n",
      "        -0.30490264, -0.15914625],\n",
      "       [-0.12916754,  0.44287652,  0.2084809 , ..., -0.07521459,\n",
      "        -0.3514044 , -0.09887037],\n",
      "       ...,\n",
      "       [ 0.09532724,  0.0388236 , -0.2344182 , ..., -0.32659563,\n",
      "        -0.10727568, -0.07862125],\n",
      "       [-0.25799647, -0.02328127, -0.10619861, ...,  0.10073078,\n",
      "         0.28008842,  0.03257942],\n",
      "       [-0.11392363,  0.20051365, -0.10578863, ..., -0.35648307,\n",
      "        -0.4311471 , -0.12027123]], dtype=float32), array([ 0.02169308, -0.07971974, -0.05559774, -0.01004806, -0.00256501,\n",
      "       -0.02971967,  0.05084664, -0.05976133, -0.03575768, -0.063921  ,\n",
      "       -0.01003077, -0.01349633,  0.01434619, -0.03217653, -0.05456308,\n",
      "       -0.01157956,  0.00958946,  0.00285666, -0.03123203, -0.01903812,\n",
      "       -0.03843132, -0.00476434, -0.00114616, -0.02983703], dtype=float32), array([[ 0.48273817,  0.2259804 , -0.5284531 , -0.04099653,  0.12090329,\n",
      "        -0.23629975, -0.00284374, -0.37903535, -0.15705387,  0.08953328,\n",
      "         0.08886963, -0.4322484 ],\n",
      "       [-0.283007  ,  0.41223618, -0.04883822,  0.45718125, -0.18880756,\n",
      "         0.05546286,  0.53470546, -0.18537596, -0.3356126 , -0.10184911,\n",
      "        -0.48551944,  0.5518227 ],\n",
      "       [-0.00824255, -0.33796334,  0.4973767 ,  0.32391205, -0.18839194,\n",
      "         0.07092661,  0.14747961,  0.26166102,  0.06540582, -0.27994987,\n",
      "        -0.18927117,  0.26463366],\n",
      "       [ 0.30971617, -0.25262535, -0.04754891, -0.18006231,  0.29251415,\n",
      "        -0.07664218, -0.2497983 ,  0.44995886, -0.00657122,  0.12303294,\n",
      "         0.00260605,  0.11654495],\n",
      "       [ 0.35955495, -0.24268515, -0.09800218, -0.02685693,  0.19313556,\n",
      "        -0.07752156, -0.00963082, -0.19706963,  0.50771356, -0.10412312,\n",
      "         0.39091215, -0.02219871],\n",
      "       [-0.09184327, -0.08816555,  0.10734492,  0.09593277, -0.25336328,\n",
      "        -0.11188705, -0.32512164,  0.4535953 ,  0.35848954, -0.09124184,\n",
      "         0.47708616,  0.0283409 ],\n",
      "       [ 0.03881768, -0.09527017, -0.21183938,  0.18048395, -0.21566267,\n",
      "         0.1298678 , -0.38189462, -0.14004996, -0.02897708, -0.029846  ,\n",
      "        -0.05607625, -0.59298646],\n",
      "       [ 0.19426946,  0.22710045, -0.50003546, -0.26548195, -0.24576706,\n",
      "         0.25305077, -0.09137274, -0.05083316, -0.37078923, -0.10739154,\n",
      "        -0.5976931 ,  0.35507038],\n",
      "       [ 0.33536473,  0.21823087, -0.08318868,  0.21517788,  0.4649534 ,\n",
      "         0.15724072,  0.32876483,  0.44713923, -0.29012886, -0.58673936,\n",
      "        -0.09150841, -0.06251623],\n",
      "       [ 0.3397608 ,  0.38105783, -0.00079532, -0.23463514,  0.00088944,\n",
      "        -0.2189904 , -0.4557767 , -0.5379919 ,  0.3194753 ,  0.2629144 ,\n",
      "         0.24204984,  0.45291275],\n",
      "       [ 0.02149712,  0.34177774,  0.00872803,  0.38868624, -0.2877912 ,\n",
      "         0.08933486, -0.39051425, -0.6044033 , -0.31990752, -0.62695897,\n",
      "        -0.2752513 ,  0.05080657],\n",
      "       [ 0.19149831, -0.3565055 , -0.25183177,  0.17997552, -0.57797587,\n",
      "         0.01020613,  0.35341355,  0.20136851,  0.09590141,  0.39104363,\n",
      "         0.04389719, -0.10563017],\n",
      "       [ 0.00142101,  0.06121881,  0.00353021,  0.02368743, -0.00662361,\n",
      "        -0.13736497, -0.25961077, -0.4109621 , -0.09600714, -0.3639785 ,\n",
      "         0.35089475, -0.33236182],\n",
      "       [ 0.28131342,  0.24877767, -0.14180735, -0.24504212,  0.35883918,\n",
      "         0.29005235, -0.07293265, -0.16525976,  0.32126644, -0.37349585,\n",
      "        -0.2577945 ,  0.21110591],\n",
      "       [ 0.3187059 , -0.21586096, -0.02011943,  0.38746363, -0.585118  ,\n",
      "         0.10815053,  0.5710633 ,  0.60512716,  0.06638144,  0.06789835,\n",
      "         0.36418602,  0.15199754],\n",
      "       [-0.14639142, -0.2181122 , -0.5391355 , -0.45408487,  0.20029369,\n",
      "         0.51025456, -0.20216207, -0.38697204, -0.42806506, -0.01078368,\n",
      "         0.08889592,  0.06347483],\n",
      "       [-0.29169214, -0.1377703 ,  0.00970873,  0.01292565, -0.35848367,\n",
      "        -0.2666122 , -0.04888332,  0.01612687, -0.01561638,  0.11736304,\n",
      "        -0.10518062, -0.15971147],\n",
      "       [-0.07934684,  0.23703404,  0.36344817,  0.26624733, -0.09960931,\n",
      "        -0.02941515, -0.30223984, -0.10950714, -0.07330212,  0.2243588 ,\n",
      "         0.23721065, -0.11172085],\n",
      "       [ 0.23913099, -0.44016865,  0.42731535, -0.23023382,  0.29505643,\n",
      "        -0.55021405, -0.09440122, -0.3452824 , -0.6129748 , -0.5317449 ,\n",
      "        -0.591505  ,  0.18257529],\n",
      "       [-0.4503939 , -0.11244371,  0.38000977,  0.03739673,  0.4316418 ,\n",
      "        -0.31421   , -0.04933177,  0.37748456,  0.20656528,  0.13564911,\n",
      "        -0.03770956, -0.06497489],\n",
      "       [-0.0595035 ,  0.21930754,  0.10880814,  0.422424  ,  0.08810326,\n",
      "         0.2926437 , -0.19630499,  0.08194221,  0.37697014, -0.19726098,\n",
      "         0.00135722,  0.264458  ],\n",
      "       [-0.15864187,  0.02850813,  0.01925712, -0.13262273, -0.4382633 ,\n",
      "         0.1272571 ,  0.39957553, -0.05266369,  0.0502102 ,  0.37458548,\n",
      "        -0.3515776 , -0.180926  ],\n",
      "       [-0.16507462, -0.2945429 ,  0.18664278,  0.06973227, -0.02717012,\n",
      "         0.5078989 , -0.63259923,  0.4508023 , -0.21748029, -0.43397853,\n",
      "         0.308973  , -0.05700818],\n",
      "       [ 0.20396774,  0.16713113, -0.22174697, -0.41665378,  0.10005171,\n",
      "        -0.22329699,  0.19026585,  0.17203279,  0.03154321, -0.17482509,\n",
      "         0.19788662,  0.32057345]], dtype=float32), array([ 0.58809865, -0.30542046, -0.9327926 , -0.77668434, -0.48104933,\n",
      "        0.13535911,  0.07317329, -0.09000687, -0.01004571, -0.50521374,\n",
      "        0.47677967, -0.14736535], dtype=float32), array([[-0.00632382],\n",
      "       [ 0.01038961],\n",
      "       [-0.6255074 ],\n",
      "       [ 0.23691007],\n",
      "       [-0.09105411],\n",
      "       [-0.03855357],\n",
      "       [ 0.09095072],\n",
      "       [ 0.04726193],\n",
      "       [ 0.11199752],\n",
      "       [-0.19358891],\n",
      "       [-0.04580928],\n",
      "       [-0.29636893]], dtype=float32), array([0.00563723], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "################### KERAS ONLY ######################\n",
    "\n",
    " \n",
    "# define hyperparameters\n",
    "n_nodes = [42,24,12,1] # number of units per layer\n",
    "batch_size = 1024\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "# CHOOSE adam or adagrad \n",
    "model = keras_NN(n_nodes=n_nodes,optimizer='sgd')\n",
    "model.fit(X_tr,ret_tr,verbose=0,epochs=100,batch_size=batch_size,\n",
    "                 validation_data=(X_val,ret_val),callbacks=[early_stopping])\n",
    "print('After fitting on the training set for 100 epochs, keras return this weight parameter') \n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prashant Maikhuri\\anaconda3\\lib\\site-packages\\hessianfree\\ffnet.py:433: UserWarning: Input dtype (float64) not equal to self.dtype (<class 'numpy.float32'>)\n",
      "  (self.inputs.dtype, self.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fitting on the training set for 100 epochs, hessian free return this weight parameter\n",
      "[array([[ 2.3042445 , -0.9528785 ,  0.48874274, ...,  1.3194114 ,\n",
      "         0.08485562,  1.4561378 ],\n",
      "       [-0.35583097,  1.2491975 ,  0.7114682 , ..., -0.7036275 ,\n",
      "         2.090405  ,  0.34680372],\n",
      "       [ 2.665232  ,  1.9462991 , -0.6751843 , ...,  0.07705186,\n",
      "        -0.4185271 , -1.3700578 ],\n",
      "       ...,\n",
      "       [-0.49847376,  0.15282175,  0.36301452, ...,  0.36784315,\n",
      "         1.33204   ,  0.57144976],\n",
      "       [ 1.7228711 ,  0.2656061 ,  1.1912862 , ...,  0.89247423,\n",
      "         1.3540462 , -0.8962603 ],\n",
      "       [ 1.3128672 ,  0.7689688 ,  1.2169102 , ..., -0.67522377,\n",
      "        -1.2082196 , -0.48314008]], dtype=float32), array([0.84678406, 1.0113026 , 0.65226203, 0.8460618 , 1.4550768 ,\n",
      "       0.63922274, 1.2413863 , 1.0704874 , 1.6226617 , 1.3157724 ,\n",
      "       0.9693455 , 1.2614727 , 0.7919319 , 0.95461243, 1.0446073 ,\n",
      "       1.3060049 , 0.8781145 , 0.9831377 , 0.44360113, 0.6226942 ,\n",
      "       0.7953134 , 1.0288403 , 0.3310838 , 1.1990032 ], dtype=float32), array([[ 1.19735467e+00,  1.28404546e+00,  4.23660308e-01,\n",
      "         5.46748996e-01, -3.59450787e-01,  4.42929983e-01,\n",
      "        -7.85674527e-03,  8.33550870e-01, -5.05503714e-01,\n",
      "        -1.79537624e-01, -1.32607472e+00, -1.18539071e+00],\n",
      "       [ 1.24922252e+00,  4.58227873e-01, -1.03681576e+00,\n",
      "        -9.58944619e-01, -1.41831183e+00,  6.47204101e-01,\n",
      "        -5.55464387e-01,  8.18017840e-01, -3.43732610e-02,\n",
      "         7.35598505e-02,  7.82747269e-01, -1.65122414e+00],\n",
      "       [ 2.49323678e+00,  1.68861508e-01, -1.78636539e+00,\n",
      "        -1.15426123e+00, -4.23171490e-01, -8.96952674e-02,\n",
      "        -1.56135693e-01,  6.18699729e-01, -1.27671826e+00,\n",
      "         1.02292746e-02,  5.36758125e-01,  6.02284491e-01],\n",
      "       [-2.00658107e+00, -1.15153372e+00,  1.15133608e-02,\n",
      "        -1.81405473e+00, -2.75995851e-01, -1.21673739e+00,\n",
      "         3.64819199e-01, -1.44667506e+00, -4.95342433e-01,\n",
      "        -2.16409843e-02, -5.24894416e-01, -5.08119583e-01],\n",
      "       [-3.74100387e-01,  8.80553126e-01,  1.31705058e+00,\n",
      "         5.40138260e-02, -1.25174546e+00, -6.91734910e-01,\n",
      "        -1.29440558e+00, -1.81643337e-01, -5.99001110e-01,\n",
      "        -3.12230206e+00, -4.54020143e-01,  1.92555517e-01],\n",
      "       [-2.61513442e-01,  1.95329583e+00, -1.73655525e-01,\n",
      "        -2.67498165e-01,  7.93758750e-01,  4.70156461e-01,\n",
      "         1.38063979e+00,  4.53500807e-01, -1.30122259e-01,\n",
      "         7.12537646e-01, -4.15094137e-01, -9.27783698e-02],\n",
      "       [ 6.19255483e-01,  2.44151741e-01, -6.91575706e-01,\n",
      "         2.64001936e-01, -3.27364802e-01,  2.92209744e-01,\n",
      "        -1.80079699e+00,  7.14028597e-01, -1.58433044e+00,\n",
      "         1.67866766e+00, -7.12878108e-01,  4.81715143e-01],\n",
      "       [ 1.88625798e-01, -6.97985888e-01, -1.44109130e+00,\n",
      "         1.43320769e-01,  7.40810573e-01, -5.12880147e-01,\n",
      "        -8.29243839e-01, -1.19688630e+00, -2.44865283e-01,\n",
      "        -8.37899983e-01, -4.71496284e-01, -6.22550428e-01],\n",
      "       [ 1.11621308e+00, -1.13305815e-01,  2.11910367e-01,\n",
      "        -1.56029797e+00, -5.21107197e-01, -1.05415010e+00,\n",
      "        -1.36878014e+00, -9.23970759e-01, -1.21426916e+00,\n",
      "         4.22257274e-01, -1.41112328e+00, -2.46292830e-01],\n",
      "       [-9.87654626e-02,  2.67648250e-01,  1.82995450e+00,\n",
      "        -1.15502548e+00,  1.42803526e+00, -6.56266570e-01,\n",
      "        -8.70644599e-02, -1.34441644e-01,  2.65762401e+00,\n",
      "        -8.76377225e-01, -1.29750133e-01, -4.58559185e-01],\n",
      "       [ 1.32951188e+00,  2.30966276e-03,  5.96716642e-01,\n",
      "        -9.42233428e-02,  1.16157341e+00,  1.91193736e+00,\n",
      "         4.54475224e-01,  3.67323458e-01, -5.45613527e-01,\n",
      "        -1.07999706e+00, -6.41117215e-01, -1.89974260e+00],\n",
      "       [-2.42262825e-01,  3.49839032e-01,  1.19800079e+00,\n",
      "         1.04671698e-02, -1.23161471e+00,  5.19248471e-02,\n",
      "        -3.32894802e-01,  1.42833069e-01,  5.37429154e-01,\n",
      "        -8.81068587e-01, -1.34381700e+00, -6.96449935e-01],\n",
      "       [ 3.68200123e-01,  4.18934822e-01, -1.47636211e+00,\n",
      "        -6.45076871e-01, -8.17784965e-02,  4.15044904e-01,\n",
      "        -3.97966146e-01, -8.48283648e-01, -1.64355576e+00,\n",
      "         5.47487557e-01,  1.42916405e+00, -8.64144087e-01],\n",
      "       [ 2.04084373e+00, -4.29008394e-01,  2.08715558e+00,\n",
      "         3.24642986e-01,  4.22333270e-01,  7.99718678e-01,\n",
      "        -2.95180321e-01, -3.05912900e+00,  3.36774498e-01,\n",
      "         8.99608135e-01, -9.19010565e-02,  8.69255736e-02],\n",
      "       [ 2.06834531e+00, -9.13445354e-01, -1.26832879e+00,\n",
      "         7.05455184e-01, -4.86081958e-01,  1.11886013e+00,\n",
      "        -1.29441953e+00, -1.16843009e+00,  1.00223625e+00,\n",
      "         2.82194048e-01, -1.17009901e-01, -6.47216260e-01],\n",
      "       [-1.34691489e+00, -5.55435836e-01,  1.55942047e+00,\n",
      "        -4.57927585e-01,  4.18666154e-01, -1.08176172e+00,\n",
      "        -2.42613494e-01, -1.73550713e+00, -7.13211522e-02,\n",
      "         9.66785848e-01, -7.37126112e-01, -6.70736432e-01],\n",
      "       [-9.67532933e-01, -3.93934637e-01,  4.37031686e-01,\n",
      "        -2.25506395e-01,  2.46637151e-01,  6.96361735e-02,\n",
      "        -8.06969851e-02,  2.35025144e+00,  2.05709368e-01,\n",
      "         1.08569160e-01, -2.43312232e-02,  5.72178841e-01],\n",
      "       [-4.60789919e-01,  5.53967692e-02, -1.50528967e+00,\n",
      "         1.23979259e+00,  4.77093726e-01, -2.25983214e+00,\n",
      "        -4.88691121e-01, -8.33853185e-01, -1.04331398e+00,\n",
      "        -1.18215919e+00,  6.13602877e-01,  7.22242072e-02],\n",
      "       [-1.78344056e-01, -6.75696850e-01, -1.82553852e+00,\n",
      "        -1.38031483e-01,  3.55231881e-01,  8.61323297e-01,\n",
      "         1.35482419e+00, -7.07861900e-01, -1.53187215e+00,\n",
      "        -9.46052551e-01,  6.05900407e-01,  6.50204897e-01],\n",
      "       [ 2.97792077e-01,  1.17011464e+00,  3.17700028e-01,\n",
      "         1.08042717e+00,  9.03957546e-01, -3.10402095e-01,\n",
      "         7.24926472e-01,  5.36636889e-01, -8.60213995e-01,\n",
      "        -1.08188182e-01, -3.56657386e-01,  1.88945785e-01],\n",
      "       [ 4.74503964e-01, -3.78253043e-01, -2.70180196e-01,\n",
      "        -2.06711799e-01,  1.33943117e+00, -1.63894236e+00,\n",
      "        -8.73574466e-02,  6.05493128e-01, -1.12503839e+00,\n",
      "         4.19132739e-01, -6.31808281e-01,  4.14929241e-01],\n",
      "       [-3.27165514e-01, -3.43733102e-01, -6.38975739e-01,\n",
      "        -1.63462389e+00, -1.86130071e+00, -4.32416350e-01,\n",
      "        -1.33189738e+00, -1.73440859e-01,  1.52304804e+00,\n",
      "         1.19588184e+00,  5.06096244e-01,  1.63167822e+00],\n",
      "       [ 2.71907207e-02, -1.87870443e-01, -8.04848909e-01,\n",
      "        -5.97424448e-01,  1.83666980e+00, -1.70792198e+00,\n",
      "         1.56222761e+00,  2.34394640e-01,  1.03618419e+00,\n",
      "        -2.47798309e-01,  1.12264347e+00, -4.12912369e-01],\n",
      "       [-1.00786316e+00,  5.74874640e-01,  3.79688829e-01,\n",
      "        -9.01662707e-01, -1.34367442e+00, -1.66346520e-01,\n",
      "        -8.90549779e-01, -1.01096749e+00, -1.08182788e+00,\n",
      "        -2.37161264e-01, -3.73883367e-01, -3.02553087e-01]], dtype=float32), array([0.9871369 , 1.03392   , 1.0895679 , 0.8724599 , 1.0402683 ,\n",
      "       0.96505475, 0.64087373, 0.97385305, 0.99913126, 0.9860593 ,\n",
      "       0.8981206 , 0.8593271 ], dtype=float32), array([[-7.0271138e-03],\n",
      "       [ 2.1585587e-02],\n",
      "       [ 6.1647771e-03],\n",
      "       [ 1.2759008e+00],\n",
      "       [ 3.3919234e-02],\n",
      "       [ 3.2817415e-04],\n",
      "       [-8.1840426e-01],\n",
      "       [ 5.7275745e-04],\n",
      "       [ 3.3279084e-02],\n",
      "       [-9.3276184e-03],\n",
      "       [ 1.2033902e+00],\n",
      "       [-6.3943821e-01]], dtype=float32), array([0.00475967], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "################### Hessian Free ######################\n",
    "\n",
    "\n",
    "\n",
    "def output_loss(func):\n",
    "    \"\"\"Convenience decorator that takes a loss defined for the output layer\n",
    "    and converts it into the more general form in terms of all layers.\"\"\"\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapped_loss(self, activities, targets):\n",
    "        result = [None for _ in activities[:-1]]\n",
    "        result += [func(self, activities[-1], targets)]\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapped_loss\n",
    "\n",
    "class mse(LossFunction):\n",
    "    \n",
    "    @output_loss\n",
    "    def loss(self, output, targets):\n",
    "        return np.sum(np.nan_to_num(output - targets) ** 2,\n",
    "                      axis=tuple(range(1, output.ndim))) / 2 /output.shape[0]\n",
    "\n",
    "    @output_loss\n",
    "    def d_loss(self, output, targets):\n",
    "        return np.nan_to_num(output - targets)/output.shape[0]\n",
    "\n",
    "    @output_loss\n",
    "    def d2_loss(self, output, _):\n",
    "        return np.ones_like(output)/output.shape[0]\n",
    "    \n",
    "def pack_weights(ff):\n",
    "    '''\n",
    "    input: an hessian free model\n",
    "    output: a list of weight following keras' format\n",
    "    ff follows this format: [(W_0,b_0),(W_1,b_1)...(W_H,b_H)]'''\n",
    "    res = []\n",
    "    for i in range(len(n_nodes)-1):\n",
    "        weights = ff.get_weights(ff.W,(i,i+1))\n",
    "        \n",
    "        res.extend([np.array(weights[0]),np.array(weights[1])])\n",
    "    return res\n",
    "\n",
    "pshape = lambda a_list: [ w.shape for w in a_list]\n",
    "\n",
    "\n",
    "# define hyperparameters\n",
    "layers = (len(n_nodes)-1)*['ReLU'] + ['Linear'] # all relu except linear for output layer\n",
    "n_nodes = [42,24,12,1] # number of units per layer\n",
    "batch_size = 1024\n",
    "\n",
    "\n",
    "# initialize a hessian free model with GPU use optional\n",
    "ff = hf.FFNet(n_nodes,layers=layers,loss_type=mse(),\n",
    "          W_init_params={ \"coeff\":1.0, \"biases\":1.0,\"init_type\":'gaussian'},use_GPU=0)\n",
    "\n",
    "ff.run_epochs(X,ret,test=(X_val,ret_val),minibatch_size=1024,\n",
    "                      optimizer=hf.opt.HessianFree(CG_iter=2),\n",
    "                      max_epochs=50, plotting=True,print_period=None)\n",
    "\n",
    "print('After fitting on the training set for 100 epochs, hessian free return this weight parameter') \n",
    "print(pack_weights(ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prashant Maikhuri\\anaconda3\\lib\\site-packages\\hessianfree\\ffnet.py:433: UserWarning: Input dtype (float64) not equal to self.dtype (<class 'numpy.float32'>)\n",
      "  (self.inputs.dtype, self.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  keras adagrad\n",
      "running time per trial [6.18560243]\n",
      "prediction scores\n",
      "accuracy\n",
      "2.5 and 97.5 percentile [0.5049827130364043,0.5049827130364043]\n",
      "hit_ratio\n",
      "2.5 and 97.5 percentile [0.5008136696501221,0.5008136696501221]\n",
      "mean_squared_error\n",
      "2.5 and 97.5 percentile [0.900225480635662,0.900225480635662]\n",
      "mean_absolute_error\n",
      "2.5 and 97.5 percentile [0.5821926761449598,0.5821926761449598]\n",
      "Evaluating  hessian free\n",
      "running time per trial [10.52531791]\n",
      "prediction scores\n",
      "accuracy\n",
      "2.5 and 97.5 percentile [0.48586536505999595,0.48586536505999595]\n",
      "hit_ratio\n",
      "2.5 and 97.5 percentile [0.37835638730675347,0.37835638730675347]\n",
      "mean_squared_error\n",
      "2.5 and 97.5 percentile [321.92099593473733,321.92099593473733]\n",
      "mean_absolute_error\n",
      "2.5 and 97.5 percentile [8.574186604401204,8.574186604401204]\n"
     ]
    }
   ],
   "source": [
    "############################## Evaluation metrics ##############################\n",
    "\n",
    "# run some number of trials for each model\n",
    "n_trials = 1\n",
    "n_nodes = [42,24,12,1] # number of units per layer\n",
    "batch_size = 1024\n",
    "layers = (len(n_nodes)-1)*['ReLU'] + ['Linear'] # all relu except linear for output layer\n",
    "\n",
    "# define evaluation metrics\n",
    "accuracy = lambda pred,truth: np.mean((pred>0)==(truth>0))\n",
    "hit_ratio = lambda x,y: np.mean( ((x[1:] - x[:-1]) * (y[1:]-y[:-1]))>0 )\n",
    "eval_f = [accuracy,hit_ratio,mean_squared_error,mean_absolute_error]\n",
    "labels = 'accuracy,hit_ratio,mean_squared_error,mean_absolute_error'.split(',')\n",
    "\n",
    "timer = np.zeros((n_trials,2))\n",
    "scores = np.zeros( (n_trials,len(labels), 2) )\n",
    "\n",
    "for i in range(n_trials):\n",
    "                      \n",
    "    # CHOOSE sgd, adam or adagrad \n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    start = time.time()\n",
    "    model = keras_NN(n_nodes=n_nodes,optimizer='sgd')\n",
    "    hist = model.fit(X_tr,ret_tr,verbose=0,epochs=100,batch_size=batch_size,\n",
    "                     validation_data=(X_val,ret_val),callbacks=[early_stopping])\n",
    "    timer[i,0] = time.time()-start\n",
    "    \n",
    "    # evaluation metrics\n",
    "    pred = model.predict(X_test).flatten()\n",
    "    truth = ret_test.flatten()\n",
    "    scores[i,:,0] = [ f(pred,truth) for j,f in enumerate(eval_f) ]\n",
    "               \n",
    "    \n",
    "    # initliaze a hessian free model\n",
    "    ff = hf.FFNet(n_nodes,layers=layers,loss_type=hf.loss_funcs.SquaredError(),\n",
    "              W_init_params={ \"coeff\":1.0, \"biases\":1.0,\"init_type\":'gaussian'},use_GPU=0)\n",
    "    \n",
    "    # Hession free\n",
    "    start = time.time()\n",
    "    ff.run_epochs(X,ret,test=(X_val,ret_val),minibatch_size=1024,\n",
    "                          optimizer=hf.opt.HessianFree(CG_iter=2),\n",
    "                          max_epochs=50, plotting=True,print_period=None)\n",
    "    timer[i,1] = time.time()-start\n",
    "    \n",
    "    # here I am borrowing Keras' model to evaluate the loss function of weights from Hessian free\n",
    "    model.set_weights(pack_weights(ff))\n",
    "    \n",
    "     # evaluation metrics\n",
    "    pred = model.predict(X_test).flatten()\n",
    "    truth = ret_test.flatten()\n",
    "    scores[i,:,1] = [ f(pred,truth) for j,f in enumerate(eval_f) ]\n",
    "    \n",
    "\n",
    "\n",
    "# print 'keras training loss',hist.history['loss']\n",
    "# print 'valdidation loss',hist.history['val_loss']\n",
    "# print 'Hessian Free training loss',ff.optimizer.plots['training error (log)'] # it says log but it's not for MSE\n",
    "# print 'Hessian Free validation loss',ff.test_errs\n",
    "\n",
    "for jj in range(2):\n",
    "    print \n",
    "    exp = 'keras adagrad,hessian free'.split(',')[jj]\n",
    "    print('Evaluating ',exp)\n",
    "    print('running time per trial',timer[:,jj])\n",
    "    s = scores[:,:,jj]\n",
    "    print('prediction scores')\n",
    "    \n",
    "    mu = s.mean(axis=0)\n",
    "    sd = s.std(axis=0)\n",
    "\n",
    "    lower_bound = np.percentile(s, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(s, 97.5, axis=0)\n",
    "     \n",
    "    for i in range(s.shape[1]):\n",
    "        print(labels[i])\n",
    "        ##print('mean {} and std {}'.format(mu[i],std[i]))\n",
    "        print('2.5 and 97.5 percentile [{},{}]'.format(lower_bound[i],upper_bound[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
